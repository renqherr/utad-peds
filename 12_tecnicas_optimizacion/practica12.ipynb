{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#U-TAD - PEDS 2015 (2ª Edición)\n",
    "#12 - Práctica de Técnicas de Optimización\n",
    "## Autor: Rafael Enríquez Herrador (rafael.enriquez@live.u-tad.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cvxpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1 (3 ptos.)\n",
    "\n",
    "A partir de los siguientes ejemplos de funciones convexas:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**1. Función constante**: $f:\\mathbb R ^n \\rightarrow \\mathbb R$ tal que $f(x)=b$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**2. Función afín**: $f:\\mathbb R ^n \\rightarrow \\mathbb R$ tal que $f(x)=a^Tx+b$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**3. Función cuadrática**: $f:\\mathbb R ^n \\rightarrow \\mathbb R$ tal que  $f(x) = \\frac{1}{2} x^T A x + b^T x + c$, siempre y cuando A sea semidefinida positiva.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**4. Normas**: $f:\\mathbb R^n \\rightarrow \\mathbb R$ tal que $f(x)=l(x)$, siendo $l$ una norma. \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ejemplos de norma: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$l_2 (x) = \\sqrt{x_{1}^2+...+x_{n}^2}$\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$l_1 (x) = |x_1|+...+|x_n|$\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$l_\\inf (x) = max(|x_1|,...,|x_n|)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**5. Exponencial**: $f:\\mathbb R \\rightarrow \\mathbb R$ tal que $f(x)=e^x$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**6. Logaritmo negado**: $f:\\mathbb R^+ \\rightarrow \\mathbb R$ tal que $f(x)=-ln(x)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**7. Suma de exponenciales**: $f:\\mathbb R^n \\rightarrow \\mathbb R$ tal que $f(x_1,...,x_n)=e^{x_1}+...+e^{x_n}$\n",
    "\n",
    "\n",
    "Y de las siguientes propiedades:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**1. Adición**: Si $f_1$ y $f_2$ son convexas, entonces $f_1+f_2$ también es convexa.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**2. Escalado**: Si $f$ es convexa, entonces $\\alpha f$ es convexa para $\\alpha > 0$, $\\alpha \\in \\mathbb R$.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**3. Transformación afín**: Si $f:\\mathbb R^n \\rightarrow \\mathbb R$  es convexa, entonces $g(x)=f(Ax+b)$ es convexa para cualquier $A \\in \\mathbb R^{n \\times m}$ y $b \\in \\mathbb R^n$.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**4. Sumar una función lineal**: Si $f$ es convexa, entonces $g(x) = f(x) + a^T x$ también es convexa para cualquier $a \\in \\mathbb R^n$.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**5. Restar una función lineal**: Si $f$ es convexa, entonces $g(x) = f(x) - a^T x$ también es convexa para cualquier $a \\in \\mathbb R^n$.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**6. Maximo punto a punto**: Si $f_1$ y $f_2$ son convexas, entonces $g(x) = max(f_1(x), f_2(x))$ también es convexa.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**7. Negación**: Si $f$ es cóncava, entonces $g(x) = -f(x)$ es convexa.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;**8. Composición escalar**: Sean $f:\\mathbb R ^n \\rightarrow \\mathbb R$ y $h:\\mathbb R \\rightarrow \\mathbb R$ entonces $g(x) = h(f(x))$ será convexa si:\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**a)** $f$ es convexa y $h$ es convexa y no decreciente.\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**b)** $f$ es concava y $h$ es convexa y no creciente.\n",
    "\n",
    "Demuestra que las siguientes funciones son convexas. Para ello sólo es necesario indicar qué ejemplos y qué propiedades es necesario utilizar en la demostración y en qué orden.\n",
    "\n",
    "**Ejemplo resuelto:** $f(x_1, x_2)= max(\\sqrt{x_1^2+x_2^2}, 1)$\n",
    "\n",
    "La demostración utilzaría en este orden el ejemplo 4, ejemplo 1 y la propiedad 6. \n",
    "\n",
    "**a)** $f(x_1,x_2)= -ln(-e^{x_1} - e^{x_2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* $f: \\mathbb R^2 \\rightarrow \\mathbb R$ tal que $f(x_1, x_2) = e^x_1  + e^x_2$ entonces $f$ es convexa puesto que se trata de una **suma de exponenciales** (**ejemplo 7**).\n",
    "* Si $f$ es convexa, entonces $g(x_1,x_2) = -f(x_1,x_2) = -(e^x_1  + e^x_2) = -e^x_1  - e^x_2$ es cóncava  puesto que se trata de la **negación** de una función convexa (**propiedad 7**).\n",
    "* Por otro lado sea $h: \\mathbb R \\rightarrow \\mathbb R$ tal que $h(x) = -ln(x)$, entonces $h$ es convexa puesto que se trata del **logaritmo negado** (**ejemplo 6**) y, además es decreciente ya que $h'(x) = -1/x$ (derivada de $h$) es negativo en todo su dominio.\n",
    "* Finalmente, por **composicion escalar** (**propiedad 8b**) tenemos que $h(g(x_1, x_2)) = -ln(-e^x_1  - e^x_2)$ será convexa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** $f(x_1, x_2) = e^{(-2x_1 + 2x_2 + 8)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Por un lado, tenemos que, sea $f: \\mathbb R^2 \\rightarrow \\mathbb R$ tal que $f(x_1, x_2) = -2x_1 + 2x_2 + 8$ entonces $f$ es convexa puesto que se trata de una **función afín** (**ejemplo 2**).\n",
    "* Por otro lado, sea $h: \\mathbb R \\rightarrow \\mathbb R$ tal que $h(x) = e^x$, entonces $h$ es convexa, puesto que se trata de la **función exponencial** (**ejemplo 5**) y además es creciente, puesto que su derivada $h'(x) = e^x$ es positiva en todo su dominio.\n",
    "* Aplicando la **composición escalar** (**propiedad 8a**), siendo $f$ convexa y $h$ convexa y no decreciente, tenemos que $h(f(x_1, x_2)) = h(-2x_1 + 2x_2 + 8) = e^{(-2x_1 + 2x_2 + 8)}$ será convexa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** $f(x_1, x_2) = 3|x_1| - 9x_1 + 3|x_2|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Sea $f_1: \\mathbb R^2 \\rightarrow \\mathbb R$ tal que $f_1(x_1, x_2) = |x_1| + |x_2|$, por tratarse de la **norma** con $p = 1$ (**ejemplo 4**), entonces $f_1$ es convexa.\n",
    "* Sea $f_2 = 3f_1$, aplicando la **propiedad de escalado** (**propiedad 2**), entonces $f_2(x_1, x_2) = 3f(x_1, x_2) = 3(|x_1| + |x_2|) = 3|x_1| + 3|x_2|$ es convexa.\n",
    "* Por otro lado, sea $g: \\mathbb R^2 \\rightarrow \\mathbb R$ tal que $g(x_1, x_2) = -9x_1 + 0x_2$, por tratarse de la **función afín** (**ejemplo 2**), es convexa.\n",
    "* Aplicando la **propiedad de adición** (**propiedad 1**), tendremos que $f_2 + g = 3|x_1| + 3|x_2| + (-9)x_1 + 0x_2 = 3|x_1| - 9x_1 + 3|x_2|$ es convexa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2 (3,5 ptos.)\n",
    "\n",
    "Resuelva el siguiente problema de optimización convexa utilizando CVXPY:\n",
    "\n",
    "<i>minimizar</i> $f(x_1,x_2)=x_1^2+9x_2^2$\n",
    "\n",
    "<i>sujeto a</i>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;$2x_1+x_2 \\ge 1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;$x_1+3x_2 \\ge 1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;$x_1 \\ge 0$, $x_2 \\ge 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: optimal\n",
      "Optimal value: 0.499999999149\n",
      "Optimal vars: 0.499996068212 0.166667977283\n"
     ]
    }
   ],
   "source": [
    "# Creamos las variables escalares de optimización\n",
    "x = cvxpy.Variable()\n",
    "y = cvxpy.Variable()\n",
    "\n",
    "# Creamos las restricciones del problema\n",
    "constraints = [2*x + y >= 1,\n",
    "               x + 3*y >= 1,\n",
    "               x >= 0,\n",
    "               y >= 0]\n",
    "\n",
    "# Ajustamos nuestra función objetivo\n",
    "obj = cvxpy.Minimize(cvxpy.square(x) + 9*cvxpy.square(y))\n",
    "\n",
    "# Solucionamos el problema\n",
    "prob = cvxpy.Problem(obj, constraints)\n",
    "prob.solve()\n",
    "\n",
    "# Imprimimos los resultados por pantalla\n",
    "print \"Status:\", prob.status\n",
    "print \"Optimal value:\", prob.value\n",
    "print \"Optimal vars:\", x.value, y.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3 (3,5 ptos.)\n",
    "\n",
    "En clase vimos un ejemplo de implementación de descenso estocástico de gradiente para regresión lineal en el caso de dos dimensiones. En este ejercicio, se pide escribir una implementación de este algoritmo que funcione para cualquier número de dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def descenso_estocastico_gradiente(x, y, theta_ini, pasadas, eta):\n",
    "    \"\"\"\n",
    "    Implementa el algoritmo de descenso estocástico de gradiente\n",
    "    para regresión lineal por mínimos cuadrados.\n",
    "    \n",
    "    Parámetros:\n",
    "    x         variable independiente de la regresión lineal. Array de dimensiones mxn\n",
    "    y         variable dependiente de la regresión lineal. Array unidimensional de m elementos\n",
    "    theta_ini valor inicial de los parámetros a optimizar. Array unidimensional de n elementos.\n",
    "    pasadas   número de veces que recorreremos cada uno de los m elementos del conjunto de datos\n",
    "    eta       longitud de paso\n",
    "    \n",
    "    Valor devuelto:\n",
    "    Array con los valores óptimos de theta. Array unidimensional de n elementos.\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: implementar esta función\n",
    "    \n",
    "    # Numero de puntos que vamos a estudiar en cada pasada\n",
    "    ## A ELECCION DEL USUARIO (con k = 4 nos acercamos bastante al valor esperado)\n",
    "    k = 4\n",
    "    \n",
    "    # Obtenemos el numero de dimensiones de nuestra funcion (numero de variables)\n",
    "    n = x.shape[1]\n",
    "    # Obtenemos el numero de puntos de nuestra muestra\n",
    "    m = x.shape[0]\n",
    "    # Creamos un vector con los indices de cada uno de los puntos que vamos a\n",
    "    # encontrar en nuestro data set\n",
    "    indexes = np.arange(m)\n",
    "    # y lo ordenamos de forma aleatoria, de forma que los puntos no se evaluen\n",
    "    # secuencialmente.\n",
    "    np.random.shuffle(indexes)\n",
    "    # Definimos el valor de theta inicial, como un vector de 0s\n",
    "    theta = theta_ini\n",
    "    # Definimos la funcion f(theta, x_i) como el sumatorio de cada elemento del vector\n",
    "    # theta, por el elemento del punto que estemos estudiando.\n",
    "    f = lambda theta,x_i: np.sum(theta * x_i)\n",
    "    # Definimos la funcion de coste\n",
    "    cost = lambda theta, x_i, y_i: ((f(theta, x_i)-y_i)**2) / n\n",
    "    \n",
    "    for i in range(pasadas):\n",
    "        sum_cost = 0\n",
    "        sample = np.take(indexes, np.random.randint(0, m, k))\n",
    "        # Obtenemos un subconjunto de indices aleatorio\n",
    "        for index in sample:\n",
    "            # Calculamos f(theta, x_i) para el punto estudiado\n",
    "            f_i = f(theta, x[index])\n",
    "            # Actualizamos el valor de theta\n",
    "            theta = theta - eta * (f_i - y[index]) * x[index]\n",
    "            # Incrementamos el coste\n",
    "            sum_cost = sum_cost + cost(theta, x[index], y[index])\n",
    "        # Almacenamos el coste y lo imprimimos por pantalla\n",
    "        # para la pasada actual\n",
    "        cost_list.append(sum_cost / m)\n",
    "        #print(\"Iteration %d | Cost: %f\" % (i, sum_cost))\n",
    "    \n",
    "    # Mostramos los valores de theta obtenidos\n",
    "    print theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código te servirá para comprobar que tu algoritmo funciona correctamente. Genera un conjunto de 100.000 datos, que siguen la función lineal $f(x_1,x_2,x_3,x_4)=-17+2x_1-8x_2+10x_3+9x_4$ pero añadiendo un ruido gaussiano a los valores de $y$.\n",
    "\n",
    "Los valores de $\\theta$ devueltos por la función que has implementado deberían ser cercanos a $(-17, 2, -8, 10, 9)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-16.41915367   1.88284685  -8.09575089  10.10493886   8.81134596]\n"
     ]
    }
   ],
   "source": [
    "m = 100000\n",
    "n = 5\n",
    "f = lambda x: 2*x[:,1]-8*x[:,2]+10*x[:,3]+9*x[:,4]-17+np.random.randn(x.shape[0])*10\n",
    "x = np.hstack((np.ones((m, 1)),np.random.random((m,n-1))*100))\n",
    "y = f(x)\n",
    "theta_ini = np.zeros(n)\n",
    "cost_list = []\n",
    "descenso_estocastico_gradiente(x, y, theta_ini, 100000, 0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
